{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW_1\n",
    "*by Vasyukevich Andrey*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk \n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "Загрузите датасет по ссылке: https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "(описание датасета можно посмотреть по ссылке\n",
    "https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "Считайте датасет в Python (можете сразу грузить все в память, выборка\n",
    "небольшая), выясните, что используется в качестве разделителей и как\n",
    "проставляются метки классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "Подготовьте для дальнейшей работы два списка: список текстов в порядке\n",
    "их следования в датасете и список соответствующих им меток классов. В\n",
    "качестве метки класса используйте 1 для спама и 0 для \"не спама\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMS Spam Collection v.1\n",
      "-------------------------\n",
      "\n",
      "1. DESCRIPTION\n",
      "--------------\n",
      "\n",
      "The SMS Spam Collection v.1 (hereafter the corpus) is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam. \n",
      "\n",
      "1.1. Compilation\n",
      "----------------\n",
      "\n",
      "This corpus has been collected from free or free for research sources at the Web:\n",
      "\n",
      "- A collection of between 425 SMS spam messages extracted manually from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: http://www.grumbletext.co.uk/\n",
      "- A list of 450 SMS ham messages collected from Caroline Tag's PhD Theses available at http://etheses.bham.ac.uk/253/1/Tagg09PhD.pdf\n",
      "- A subset of 3,375 SMS ham messages of the NUS SMS Corpus (NSC), which is a corpus of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/\n",
      "- The amount of 1,002 SMS ham messages and 322 spam messages extracted from the SMS Spam Corpus v.0.1 Big created by Jos� Mar�a G�mez Hidalgo and public available at: http://www.esp.uem.es/jmgomez/smsspamcorpus/\n",
      "\n",
      "\n",
      "1.2. Statistics\n",
      "---------------\n",
      "\n",
      "There is one collection:\n",
      "\n",
      "- The SMS Spam Collection v.1 (text file: smsspamcollection) has a total of 4,827 SMS legitimate messages (86.6%) and a total of 747 (13.4%) spam messages.\n",
      "\n",
      "\n",
      "1.3. Format\n",
      "-----------\n",
      "\n",
      "The files contain one message per line. Each line is composed by two columns: one with label (ham or spam) and other with the raw text. Here are some examples:\n",
      "\n",
      "ham   What you doing?how are you?\n",
      "ham   Ok lar... Joking wif u oni...\n",
      "ham   dun say so early hor... U c already then say...\n",
      "ham   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*\n",
      "ham   Siva is in hostel aha:-.\n",
      "ham   Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.\n",
      "spam   FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop\n",
      "spam   Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B\n",
      "spam   URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU\n",
      "\n",
      "Note: messages are not chronologically sorted.\n",
      "\n",
      "\n",
      "2. USAGE\n",
      "--------\n",
      "\n",
      "We offer a comprehensive study of this corpus in the following paper that is under review. This work presents a number of statistics, studies and baseline results for several machine learning methods.\n",
      "\n",
      "[1] Almeida, T.A., G�mez Hidalgo, J.M., Yamakami, A. Contributions to the study of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (ACM DOCENG'11), Mountain View, CA, USA, 2011. (Under review)\n",
      "\n",
      "\n",
      "3. ABOUT\n",
      "--------\n",
      "\n",
      "The corpus has been collected by Tiago Agostinho de Almeida (http://www.dt.fee.unicamp.br/~tiago) and Jos� Mar�a G�mez Hidalgo (http://www.esp.uem.es/jmgomez).\n",
      "\n",
      "We would like to thank Dr. Min-Yen Kan (http://www.comp.nus.edu.sg/~kanmy/) and his team for making the NUS SMS Corpus available. See: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/. He is currently collecting a bigger SMS corpus at: http://wing.comp.nus.edu.sg:8080/SMSCorpus/\n",
      "\n",
      "4. LICENSE/DISCLAIMER\n",
      "---------------------\n",
      "\n",
      "We would appreciate if:\n",
      "\n",
      "- In case you find this corpus useful, please make a reference to previous paper and the web page: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers, research, etc.\n",
      "- Send us a message to tiago@dt.fee.unicamp.br in case you make use of the corpus.\n",
      "\n",
      "The SMS Spam Collection v.1 is provided for free and with no limitations excepting:\n",
      "\n",
      "1. Tiago Agostinho de Almeida and Jos� Mar�a G�mez Hidalgo hold the copyrigth (c) for the SMS Spam Collection v.1.\n",
      "\n",
      "2. No Warranty/Use At Your Risk. THE CORPUS IS MADE AT NO CHARGE. ACCORDINGLY, THE CORPUS IS PROVIDED `AS IS,' WITHOUT WARRANTY OF ANY KIND, INCLUDING WITHOUT LIMITATION THE WARRANTIES THAT THEY ARE MERCHANTABLE, FIT FOR A PARTICULAR PURPOSE OR NON-INFRINGING. YOU ARE SOLELY RESPONSIBLE FOR YOUR USE, DISTRIBUTION, MODIFICATION, REPRODUCTION AND PUBLICATION OF THE CORPUS AND ANY DERIVATIVE WORKS THEREOF BY YOU AND ANY OF YOUR SUBLICENSEES (COLLECTIVELY, `YOUR CORPUS USE'). THE ENTIRE RISK AS TO YOUR CORPUS USE IS BORNE BY YOU. YOU AGREE TO INDEMNIFY AND HOLD THE COPYRIGHT HOLDERS, AND THEIR AFFILIATES HARMLESS FROM ANY CLAIMS ARISING FROM OR RELATING TO YOUR CORPUS USE.\n",
      "\n",
      "3. Limitation of Liability. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR THEIR AFFILIATES, OR THE CORPUS CONTRIBUTING EDITORS, BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES, INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOSS OF GOODWILL OR ANY AND ALL OTHER COMMERCIAL DAMAGES OR LOSSES, EVEN IF ADVISED OF THE POSSIBILITY THEREOF, AND REGARDLESS OF WHETHER ANY CLAIM IS BASED UPON ANY CONTRACT, TORT OR OTHER LEGAL OR EQUITABLE THEORY, RELATING OR ARISING FROM THE CORPUS, YOUR CORPUS USE OR THIS LICENSE AGREEMENT.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('readme') as readme:\n",
    "    print readme.read()\n",
    "    readme.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = open('SMSSpamCollection')\n",
    "dataset_lines = filter(None, dataset.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels, texts = zip(*map(lambda line: line.split('\\t', 1), dataset_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = np.asarray(map(lambda label: (label == 'spam'), labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = np.asarray(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "Используя sklearn.feature_extraction.text.CountVectorizer со стандартными\n",
    "настройками, получите из списка текстов матрицу признаков X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data = count_vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\n",
    "Оцените качество классификации текстов с помощью LogisticRegression() с\n",
    "параметрами по умолчанию, используя sklearn.cross_validation.cross_val_score и посчитав среднее арифметическое\n",
    "качества на отдельных fold'ах. Параметр cv задайте равным 10. В качестве\n",
    "метрики качества используйте f1-меру класса 1 (то есть, без микро и макро\n",
    "усреднения). Получившееся качество – ответ в этом пункте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = cross_val_score(estimator, X_data, labels, scoring='f1', cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93334852685794145"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(score.flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\n",
    "А теперь обучите классификатор на всей выборке и спрогнозируйте с его\n",
    "помощью класс для следующих сообщений:\n",
    "\"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GB\"\n",
    "\"FreeMsg: Txt: claim your reward of 3 hours talk time\"\n",
    "\"Have you visited the last lecture on physics?\"\n",
    "\"Have you visited the last lecture on physics? Just buy this book and you will have all materials! Only 99$\"\n",
    "\"Only 99$\"\n",
    "Выпишите через пробел прогнозы классификатора (0 – не спам, 1 – спам)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = LogisticRegression()\n",
    "estimator.fit(X_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GB\",\n",
    "    \"FreeMsg: Txt: claim your reward of 3 hours talk time\",\n",
    "    \"Have you visited the last lecture on physics?\",\n",
    "    \"Have you visited the last lecture on physics? Just buy this book and you will have all materials! Only 99$\",\n",
    "    \"Only 99$\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0 0 0\n"
     ]
    }
   ],
   "source": [
    "print ' '.join(map(lambda b: str(int(b)), estimator.predict(count_vectorizer.transform(messages))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\n",
    "Задайте в CountVectorizer параметр ngram_range=(2,2), затем\n",
    "ngram_range=(3,3), затем ngram_range=(1,3). Во всех трех случаях измерьте\n",
    "получившееся в кросс-валидации значение f1-меры, округлите до второго\n",
    "знака после точки, и выпишете результаты через пробел в том же порядке.\n",
    "В данном эксперименте мы пробовали добавлять в признаки n-граммы для\n",
    "разных диапазонов n - только биграммы, только триграммы, и, наконец, все\n",
    "вместе - униграммы, биграммы и триграммы. Обратите внимание, что\n",
    "статистики по биграммам и триграммам намного меньше, поэтому\n",
    "классификатор только на них работает хуже. В то же время это не ухудшает\n",
    "результат сколько-нибудь существенно, если добавлять их вместе с\n",
    "униграммами, т.к. за счет регуляризации линейный классификатор не\n",
    "склонен сильно переобучаться на этих признаках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_ngram_test(args, texts, labels, estimator, Vectorizer=CountVectorizer):\n",
    "    count_vectorizer = Vectorizer(**args)\n",
    "    X_data = count_vectorizer.fit_transform(texts)\n",
    "    score = np.mean(cross_val_score(estimator, X_data, labels, scoring='f1', cv=10).flat)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args_list = [ { 'ngram_range': (2, 2) },  { 'ngram_range': (3, 3) },  { 'ngram_range': (1, 3) } ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = [cv_ngram_test(args, texts, labels, LogisticRegression()) for args in args_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82 0.73 0.93\n"
     ]
    }
   ],
   "source": [
    "print ' '.join(map(lambda score: '%.2f' % score, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\n",
    "Повторите аналогичный п.7 эксперимент, используя вместо логистической\n",
    "регрессии MultinomialNB(). Обратите внимание, насколько сильнее (по\n",
    "сравнению с линейным классификатором) наивный Байес страдает от\n",
    "нехватки статистики по биграммам и триграммам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92709424554224729"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ngram_test({}, texts, labels, MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = [cv_ngram_test(args, texts, labels, MultinomialNB()) for args in args_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65 0.38 0.89\n"
     ]
    }
   ],
   "source": [
    "print ' '.join(map(lambda score: '%.2f' % score, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\n",
    "Попробуйте использовать в логистической регрессии в качестве признаков\n",
    "Tf*idf из TfidfVectorizer на униграммах (рекомендуется использовать вместе\n",
    "с sklearn.pipeline.Pipeline). Повысилось или понизилось качество на\n",
    "кросс-валидации по сравнению с CountVectorizer на униграммах? Обратите\n",
    "внимание, что результат перехода к tf*idf не всегда будет таким - если вы\n",
    "наблюдаете какое-то явление на одном датасете, не надо сразу же его\n",
    "обобщать на любые данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93334852685794145"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ngram_test({}, texts, labels, LogisticRegression(), CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85285995541724557"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ngram_test({}, texts, labels, LogisticRegression(), TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Качество понизилось по сравнению с CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\n",
    "Попробуйте получить как можно более высокое качество на\n",
    "кросс-валидации. Обязательно уделите внимание подбору\n",
    "макропараметров классификаторов (для этого используйте\n",
    "sklearn.model_selection.GridSearchCV). Напишите, что пробовали и какое\n",
    "качество получилось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Потестируем RandomForestClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89003150314339818"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ngram_test({}, texts, labels, RandomForestClassifier(), CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89179309757439262"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ngram_test({}, texts, labels, RandomForestClassifier(), TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Для RandomForestClassifier TfidfVectorizer показал чуть лучшие результаты!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90902779755333418"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ngram_test({}, texts, labels, RandomForestClassifier(n_estimators=50, max_features='sqrt'), TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Попробуем перебирать параметры для LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': np.logspace(-2, 5, num=30),\n",
    "    'class_weight': ['balanced'],\n",
    "    'max_iter': [100, 1000, 10000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gscv = GridSearchCV(estimator=LogisticRegression(), param_grid=param_grid, scoring='f1', cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': array([  1.00000e-02,   1.74333e-02,   3.03920e-02,   5.29832e-02,\n",
       "         9.23671e-02,   1.61026e-01,   2.80722e-01,   4.89390e-01,\n",
       "         8.53168e-01,   1.48735e+00,   2.59294e+00,   4.52035e+00,\n",
       "         7.88046e+00,   1.37382e+01,   2.39503e+01,   4.1...         5.73615e+04,   1.00000e+05]), 'max_iter': [100, 1000, 10000], 'class_weight': ['balanced']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.fit(X_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est = gscv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 41.753189365604044,\n",
       " 'class_weight': 'balanced',\n",
       " 'max_iter': 100,\n",
       " 'penalty': 'l2'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.952030378413\n"
     ]
    }
   ],
   "source": [
    "print cv_ngram_test({}, texts, labels, est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Наилучшую точность 0.952 дала LogisticRegression, судя по всему на улучшение точности повлияла установка\n",
    "# class_weight='balanced' и оптимизация параметра регуляризации C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\n",
    "Какие наблюдения и выводы можно сделать из этого задания?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение по сути велось только по ключевым словам, не задействуя синтаксическую структуру и n-граммы.\n",
    "И по такому набору удалось достичь точности 0.952, следовательно, подтверждается естественная гипотеза,\n",
    "что спам обладает некототорой характерной лексикой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
